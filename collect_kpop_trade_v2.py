#!/usr/bin/env python3
"""
ğŸµ K-pop í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ í†µí•© ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ v2

SerpAPI + Reddit APIë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python collect_kpop_trade_v2.py                      # ì„¸ë¸í‹´ ê¸°ë³¸ ìˆ˜ì§‘
    python collect_kpop_trade_v2.py --artist "BTS"       # ë‹¤ë¥¸ ì•„ì´ëŒ
    python collect_kpop_trade_v2.py --limit 200          # ìˆ˜ì§‘ ê°œìˆ˜ ì¡°ì •
    python collect_kpop_trade_v2.py --source both        # ë‘ API ëª¨ë‘ ì‚¬ìš© (ê¸°ë³¸ê°’)
    python collect_kpop_trade_v2.py --source reddit      # Reddit APIë§Œ ì‚¬ìš©
    python collect_kpop_trade_v2.py --source serpapi     # SerpAPIë§Œ ì‚¬ìš©
"""

import argparse
import json
import os
import time
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import List, Optional

import requests
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# ============================================================
# ë°ì´í„° ëª¨ë¸
# ============================================================

class SearchSource(str, Enum):
    """ê²€ìƒ‰ ì†ŒìŠ¤"""
    REDDIT = "reddit"
    REDDIT_API = "reddit_api"
    SERPAPI = "serpapi"


class TradePost(BaseModel):
    """ê±°ë˜ ê²Œì‹œê¸€ ëª¨ë¸"""
    url: str = Field(..., description="ê²Œì‹œê¸€ URL")
    title: str = Field(..., description="ì œëª©")
    content: str = Field(default="", description="ë³¸ë¬¸ ë‚´ìš©")
    snippet: str = Field(default="", description="ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°")
    author: Optional[str] = Field(default=None, description="ì‘ì„±ì")
    subreddit: Optional[str] = Field(default=None, description="ì„œë¸Œë ˆë”§")
    source: str = Field(..., description="ìˆ˜ì§‘ ì†ŒìŠ¤")
    lang: str = Field(default="en", description="ì–¸ì–´ ì½”ë“œ")
    created_at: Optional[datetime] = Field(default=None, description="ì‘ì„± ì‹œê°„")
    score: int = Field(default=0, description="ì—…ë³´íŠ¸ ìˆ˜")
    num_comments: int = Field(default=0, description="ëŒ“ê¸€ ìˆ˜")
    queried_at: datetime = Field(default_factory=datetime.now, description="ìˆ˜ì§‘ ì‹œê°„")


# ============================================================
# Reddit API í´ë˜ìŠ¤
# ============================================================

class RedditAPIClient:
    """Reddit OAuth API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.app_id = os.getenv("REDDIT_APP_ID")
        self.secret = os.getenv("REDDIT_SECRET")
        self.user_agent = "kpop-trade-collector/2.0.0 (by /u/kpop_collector)"
        self.access_token = None
        self.token_expires_at = None

    def is_available(self) -> bool:
        """Reddit API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return bool(self.app_id and self.secret)

    def authenticate(self) -> bool:
        """Reddit OAuth ì¸ì¦"""
        if not self.is_available():
            return False

        # í† í°ì´ ì•„ì§ ìœ íš¨í•˜ë©´ ì¬ì‚¬ìš©
        if self.access_token and self.token_expires_at:
            if datetime.now() < self.token_expires_at:
                return True

        try:
            auth = requests.auth.HTTPBasicAuth(self.app_id, self.secret)
            data = {
                "grant_type": "client_credentials",
                "device_id": "kpop_trade_collector_v2",
            }
            headers = {"User-Agent": self.user_agent}

            response = requests.post(
                "https://www.reddit.com/api/v1/access_token",
                auth=auth,
                data=data,
                headers=headers,
                timeout=30,
            )
            response.raise_for_status()

            token_data = response.json()
            self.access_token = token_data["access_token"]
            expires_in = token_data.get("expires_in", 3600)
            self.token_expires_at = datetime.now() + timedelta(seconds=expires_in - 60)

            return True

        except Exception as e:
            print(f"  âš ï¸ Reddit ì¸ì¦ ì‹¤íŒ¨: {e}")
            return False

    @retry(
        retry=retry_if_exception_type((requests.exceptions.RequestException, TimeoutError)),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
    )
    def search_subreddit(
        self,
        subreddit: str,
        query: str,
        limit: int = 100,
        sort: str = "relevance",
        time_filter: str = "year",
    ) -> List[TradePost]:
        """ì„œë¸Œë ˆë”§ì—ì„œ ê²€ìƒ‰"""
        if not self.access_token:
            if not self.authenticate():
                return []

        headers = {
            "Authorization": f"bearer {self.access_token}",
            "User-Agent": self.user_agent,
        }

        params = {
            "q": query,
            "limit": min(limit, 100),
            "sort": sort,
            "t": time_filter,
            "restrict_sr": True,
        }

        url = f"https://oauth.reddit.com/r/{subreddit}/search"

        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
        except Exception as e:
            print(f"    âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨ r/{subreddit}: {e}")
            return []

        posts = []
        six_months_ago = datetime.now() - timedelta(days=180)

        for post in data.get("data", {}).get("children", []):
            post_data = post.get("data", {})
            created_at = datetime.fromtimestamp(post_data.get("created_utc", 0))

            # 6ê°œì›” ì´ë‚´ ê²Œì‹œê¸€ë§Œ
            if created_at < six_months_ago:
                continue

            trade_post = TradePost(
                url=f"https://reddit.com{post_data.get('permalink', '')}",
                title=post_data.get("title", ""),
                content=post_data.get("selftext", "")[:500],  # ë³¸ë¬¸ 500ì ì œí•œ
                snippet=post_data.get("selftext", "")[:200],
                author=post_data.get("author"),
                subreddit=subreddit,
                source="reddit_api",
                lang="en",
                created_at=created_at,
                score=post_data.get("score", 0),
                num_comments=post_data.get("num_comments", 0),
            )
            posts.append(trade_post)

        return posts

    def get_new_posts(self, subreddit: str, limit: int = 100) -> List[TradePost]:
        """ì„œë¸Œë ˆë”§ ìµœì‹  ê²Œì‹œê¸€ ê°€ì ¸ì˜¤ê¸°"""
        if not self.access_token:
            if not self.authenticate():
                return []

        headers = {
            "Authorization": f"bearer {self.access_token}",
            "User-Agent": self.user_agent,
        }

        params = {"limit": min(limit, 100)}
        url = f"https://oauth.reddit.com/r/{subreddit}/new"

        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
        except Exception as e:
            print(f"    âš ï¸ ìµœì‹  ê²Œì‹œê¸€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []

        posts = []
        for post in data.get("data", {}).get("children", []):
            post_data = post.get("data", {})
            created_at = datetime.fromtimestamp(post_data.get("created_utc", 0))

            trade_post = TradePost(
                url=f"https://reddit.com{post_data.get('permalink', '')}",
                title=post_data.get("title", ""),
                content=post_data.get("selftext", "")[:500],
                snippet=post_data.get("selftext", "")[:200],
                author=post_data.get("author"),
                subreddit=subreddit,
                source="reddit_api",
                lang="en",
                created_at=created_at,
                score=post_data.get("score", 0),
                num_comments=post_data.get("num_comments", 0),
            )
            posts.append(trade_post)

        return posts


# ============================================================
# SerpAPI í´ë˜ìŠ¤
# ============================================================

class SerpAPIClient:
    """SerpAPI í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.api_key = os.getenv("SERPAPI_KEY")
        self.base_url = "https://serpapi.com/search"

    def is_available(self) -> bool:
        """SerpAPI ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return bool(self.api_key)

    @retry(
        retry=retry_if_exception_type((requests.exceptions.RequestException, TimeoutError)),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
    )
    def search(
        self,
        query: str,
        language: str = "en",
        max_results: int = 10,
    ) -> List[TradePost]:
        """Google ê²€ìƒ‰ (Reddit ì‚¬ì´íŠ¸ í•„í„°)"""
        if not self.is_available():
            return []

        params = {
            "q": f"{query} site:reddit.com",
            "api_key": self.api_key,
            "num": min(max_results, 100),
            "hl": language,
            "gl": "kr" if language == "ko" else "us",
            "tbs": "qdr:m6",  # ìµœê·¼ 6ê°œì›”
        }

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            if "error" in data:
                print(f"    âš ï¸ SerpAPI ì˜¤ë¥˜: {data.get('error')}")
                return []

        except Exception as e:
            print(f"    âš ï¸ SerpAPI ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

        posts = []
        for item in data.get("organic_results", []):
            post = TradePost(
                url=item.get("link", ""),
                title=item.get("title", ""),
                snippet=item.get("snippet", ""),
                source="serpapi",
                lang=language,
            )
            posts.append(post)

        return posts


# ============================================================
# í†µí•© ìˆ˜ì§‘ê¸°
# ============================================================

class KpopTradeCollector:
    """K-pop í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ í†µí•© ìˆ˜ì§‘ê¸°"""

    # K-pop ê±°ë˜ ê´€ë ¨ ì„œë¸Œë ˆë”§
    SUBREDDITS = [
        "kpopforsale",
        "kpopcollections",
        "kpoptrade",
        "adultkpopfans",
    ]

    # ê±°ë˜ í‚¤ì›Œë“œ
    TRADE_KEYWORDS = [
        "wts", "wtb", "wtt", "trade", "trading", "selling", "buying",
        "for sale", "iso", "ì–‘ë„", "íŒë§¤", "êµ¬í•´", "ì‚½ë‹ˆë‹¤", "íŒë‹ˆë‹¤", "êµí™˜"
    ]

    def __init__(self):
        self.reddit = RedditAPIClient()
        self.serpapi = SerpAPIClient()

    def get_search_queries(self, artist: str) -> dict:
        """ì•„í‹°ìŠ¤íŠ¸ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        return {
            "reddit_api": [
                f"{artist} photocard",
                f"{artist} pc",
                f"{artist} WTS",
                f"{artist} WTB",
                f"{artist} WTT",
                f"{artist} trade",
                f"{artist} selling",
            ],
            "serpapi": [
                f"WTS {artist} photocard",
                f"WTB {artist} photocard",
                f"WTT {artist} photocard",
                f"{artist} í¬í† ì¹´ë“œ ì–‘ë„",
                f"kpopforsale {artist}",
            ],
        }

    def is_trade_post(self, post: TradePost) -> bool:
        """ê±°ë˜ ê´€ë ¨ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸"""
        combined = (post.title + " " + post.snippet + " " + post.content).lower()
        return any(kw in combined for kw in self.TRADE_KEYWORDS)

    def contains_artist(self, post: TradePost, artist: str) -> bool:
        """ì•„í‹°ìŠ¤íŠ¸ ê´€ë ¨ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸"""
        artist_lower = artist.lower()
        # ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„ ë³€í˜• (ì˜ˆ: Seventeen -> svt, ì„¸ë¸í‹´)
        artist_variants = [artist_lower]
        
        # ì£¼ìš” ì•„í‹°ìŠ¤íŠ¸ ë³„ëª… ë§¤í•‘
        artist_aliases = {
            "seventeen": ["svt", "ì„¸ë¸í‹´", "sebong"],
            "bts": ["ë°©íƒ„ì†Œë…„ë‹¨", "bangtan"],
            "twice": ["íŠ¸ì™€ì´ìŠ¤"],
            "blackpink": ["ë¸”ë™í•‘í¬", "ë¸”í•‘"],
            "stray kids": ["skz", "ìŠ¤íŠ¸ë ˆì´í‚¤ì¦ˆ", "ìŠ¤í‚¤ì¦ˆ"],
            "newjeans": ["ë‰´ì§„ìŠ¤", "nj"],
            "aespa": ["ì—ìŠ¤íŒŒ"],
            "nct": ["ì—”ì‹œí‹°"],
            "exo": ["ì—‘ì†Œ"],
            "red velvet": ["ë ˆë“œë²¨ë²³", "ë ˆë²¨"],
            "itzy": ["ìˆì§€"],
            "txt": ["íˆ¬ëª¨ë¡œìš°ë°”ì´íˆ¬ê²Œë”", "tomorrow x together"],
            "enhypen": ["ì—”í•˜ì´í”ˆ"],
            "ive": ["ì•„ì´ë¸Œ"],
            "le sserafim": ["ë¥´ì„¸ë¼í•Œ"],
        }
        
        # ë³„ëª… ì¶”ê°€
        if artist_lower in artist_aliases:
            artist_variants.extend(artist_aliases[artist_lower])
        
        combined = (post.title + " " + post.snippet + " " + post.content).lower()
        return any(variant in combined for variant in artist_variants)

    def collect_from_reddit_api(self, artist: str, limit: int = 200) -> List[TradePost]:
        """Reddit APIë¡œ ìˆ˜ì§‘"""
        if not self.reddit.is_available():
            print("  âš ï¸ Reddit API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        if not self.reddit.authenticate():
            return []

        print("  âœ… Reddit API ì¸ì¦ ì„±ê³µ")

        queries = self.get_search_queries(artist)["reddit_api"]
        all_posts = []

        # ê° ì„œë¸Œë ˆë”§ì—ì„œ ê²€ìƒ‰
        for subreddit in self.SUBREDDITS:
            print(f"\n  ğŸ“ r/{subreddit}")

            # ìµœì‹  ê²Œì‹œê¸€ ê°€ì ¸ì˜¤ê¸°
            print(f"    [new] ìµœì‹  ê²Œì‹œê¸€...")
            new_posts = self.reddit.get_new_posts(subreddit, limit=50)
            all_posts.extend(new_posts)
            print(f"    âœ… {len(new_posts)} posts")
            time.sleep(1)  # Rate limit

            # í‚¤ì›Œë“œ ê²€ìƒ‰
            for query in queries[:3]:  # ìƒìœ„ 3ê°œ ì¿¼ë¦¬ë§Œ
                print(f"    [search] '{query}'...")
                search_posts = self.reddit.search_subreddit(subreddit, query, limit=25)
                all_posts.extend(search_posts)
                print(f"    âœ… {len(search_posts)} posts")
                time.sleep(1)  # Rate limit

        return all_posts

    def collect_from_serpapi(self, artist: str, limit: int = 100) -> List[TradePost]:
        """SerpAPIë¡œ ìˆ˜ì§‘"""
        if not self.serpapi.is_available():
            print("  âš ï¸ SERPAPI_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        print("  âœ… SerpAPI ì‚¬ìš© ê°€ëŠ¥")

        queries = self.get_search_queries(artist)["serpapi"]
        all_posts = []

        for query in queries:
            print(f"    [search] '{query}'...")
            posts = self.serpapi.search(query, language="en", max_results=10)
            all_posts.extend(posts)
            print(f"    âœ… {len(posts)} posts")

        return all_posts

    def collect(
        self,
        artist: str = "Seventeen",
        limit: int = 200,
        source: str = "both",
    ) -> List[TradePost]:
        """í†µí•© ìˆ˜ì§‘ ì‹¤í–‰"""
        print("=" * 60)
        print(f"ğŸµ {artist} í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ í†µí•© ìˆ˜ì§‘ v2")
        print("=" * 60)
        print(f"ğŸ¯ Target: WTS/WTB/WTT ê±°ë˜ ê²Œì‹œê¸€")
        print(f"ğŸ“Š Limit: ~{limit} posts")
        print(f"ğŸ”§ Source: {source}")
        print()

        all_posts = []

        # Reddit API ìˆ˜ì§‘
        if source in ["both", "reddit"]:
            print("ğŸ“¡ [1/2] Reddit API ìˆ˜ì§‘ ì¤‘...")
            reddit_posts = self.collect_from_reddit_api(artist, limit)
            all_posts.extend(reddit_posts)
            print(f"\n  ğŸ“Š Reddit API ê²°ê³¼: {len(reddit_posts)}ê°œ")

        # SerpAPI ìˆ˜ì§‘
        if source in ["both", "serpapi"]:
            print("\nğŸ” [2/2] SerpAPI ìˆ˜ì§‘ ì¤‘...")
            serp_posts = self.collect_from_serpapi(artist, limit)
            all_posts.extend(serp_posts)
            print(f"\n  ğŸ“Š SerpAPI ê²°ê³¼: {len(serp_posts)}ê°œ")

        # ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_posts = []
        for post in all_posts:
            # URL ì •ê·œí™” (trailing slash ì œê±°)
            normalized_url = post.url.rstrip("/")
            if normalized_url not in seen_urls:
                unique_posts.append(post)
                seen_urls.add(normalized_url)

        print(f"\nğŸ“Š ì¤‘ë³µ ì œê±° í›„: {len(unique_posts)}ê°œ")

        # ì•„í‹°ìŠ¤íŠ¸ í•„í„°ë§
        artist_posts = [p for p in unique_posts if self.contains_artist(p, artist)]
        print(f"ğŸ¤ ì•„í‹°ìŠ¤íŠ¸ '{artist}' í•„í„° í›„: {len(artist_posts)}ê°œ")

        # ê±°ë˜ í‚¤ì›Œë“œ í•„í„°ë§
        trade_posts = [p for p in artist_posts if self.is_trade_post(p)]
        print(f"ğŸ” ê±°ë˜ í‚¤ì›Œë“œ í•„í„° í›„: {len(trade_posts)}ê°œ")

        # ì œí•œ ì ìš©
        if len(trade_posts) > limit:
            trade_posts = trade_posts[:limit]

        return trade_posts

    def save_to_jsonl(self, posts: List[TradePost], artist: str) -> Path:
        """JSONL íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        artist_safe = artist.lower().replace(" ", "_")
        filename = Path("data") / f"{artist_safe}_trade_v2_{timestamp}.jsonl"
        Path("data").mkdir(exist_ok=True)

        with open(filename, "w", encoding="utf-8") as f:
            for post in posts:
                data = {
                    "url": post.url,
                    "title": post.title,
                    "content": post.content,
                    "snippet": post.snippet,
                    "author": post.author,
                    "subreddit": post.subreddit,
                    "source": post.source,
                    "lang": post.lang,
                    "created_at": post.created_at.isoformat() if post.created_at else None,
                    "score": post.score,
                    "num_comments": post.num_comments,
                    "queried_at": post.queried_at.isoformat(),
                }
                f.write(json.dumps(data, ensure_ascii=False) + "\n")

        return filename


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="K-pop í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ í†µí•© ìˆ˜ì§‘ v2",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python collect_kpop_trade_v2.py                       # ì„¸ë¸í‹´ ìˆ˜ì§‘
  python collect_kpop_trade_v2.py --artist "BTS"        # BTS ìˆ˜ì§‘
  python collect_kpop_trade_v2.py --artist "TWICE"      # íŠ¸ì™€ì´ìŠ¤ ìˆ˜ì§‘
  python collect_kpop_trade_v2.py --limit 300           # 300ê°œê¹Œì§€ ìˆ˜ì§‘
  python collect_kpop_trade_v2.py --source reddit       # Reddit APIë§Œ ì‚¬ìš©
  python collect_kpop_trade_v2.py --source serpapi      # SerpAPIë§Œ ì‚¬ìš©
  python collect_kpop_trade_v2.py --source both         # ë‘˜ ë‹¤ ì‚¬ìš© (ê¸°ë³¸ê°’)
        """
    )

    parser.add_argument(
        "--artist",
        type=str,
        default="Seventeen",
        help="ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„ (ê¸°ë³¸: Seventeen)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=200,
        help="ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸: 200)",
    )
    parser.add_argument(
        "--source",
        type=str,
        choices=["both", "reddit", "serpapi"],
        default="both",
        help="ë°ì´í„° ì†ŒìŠ¤ (ê¸°ë³¸: both)",
    )

    args = parser.parse_args()

    # ìˆ˜ì§‘ ì‹¤í–‰
    collector = KpopTradeCollector()
    posts = collector.collect(args.artist, args.limit, args.source)

    if not posts:
        print("\nâŒ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ API í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”:")
        print("   - REDDIT_APP_ID, REDDIT_SECRET (.env)")
        print("   - SERPAPI_KEY (.env)")
        return

    # ì €ì¥
    filename = collector.save_to_jsonl(posts, args.artist)

    print(f"\n{'=' * 60}")
    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(posts)}ê°œ ê±°ë˜ ê²Œì‹œê¸€")
    print(f"ğŸ’¾ ì €ì¥: {filename}")
    print("=" * 60)

    # ì†ŒìŠ¤ë³„ í†µê³„
    sources = {}
    for post in posts:
        sources[post.source] = sources.get(post.source, 0) + 1

    print("\nğŸ“Š ì†ŒìŠ¤ë³„ í†µê³„:")
    for source, count in sorted(sources.items(), key=lambda x: -x[1]):
        print(f"  - {source}: {count}ê°œ")

    # ìƒ˜í”Œ ì¶œë ¥
    print("\nğŸ“‹ ìˆ˜ì§‘ëœ ê±°ë˜ ê²Œì‹œê¸€ ìƒ˜í”Œ:")
    for i, post in enumerate(posts[:10], 1):
        title = post.title[:50] + "..." if len(post.title) > 50 else post.title
        source_tag = f"[{post.source}]"
        print(f"  {i}. {source_tag} {title}")

    if len(posts) > 10:
        print(f"  ... ì™¸ {len(posts) - 10}ê°œ")


if __name__ == "__main__":
    main()
