#!/usr/bin/env python3
"""
ğŸµ K-pop í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ í†µí•© ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ v2

SerpAPI + Reddit APIë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ë” ë§ì€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python collect_kpop_trade_v2.py                      # ì„¸ë¸í‹´ ê¸°ë³¸ ìˆ˜ì§‘
    python collect_kpop_trade_v2.py --artist "BTS"       # ë‹¤ë¥¸ ì•„ì´ëŒ
    python collect_kpop_trade_v2.py --limit 200          # ìˆ˜ì§‘ ê°œìˆ˜ ì¡°ì •
    python collect_kpop_trade_v2.py --source both        # ë‘ API ëª¨ë‘ ì‚¬ìš© (ê¸°ë³¸ê°’)
    python collect_kpop_trade_v2.py --source reddit      # Reddit APIë§Œ ì‚¬ìš©
    python collect_kpop_trade_v2.py --source serpapi     # SerpAPIë§Œ ì‚¬ìš©
"""

import argparse
import json
import os
import time
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import List, Optional

import requests
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# ============================================================
# ë°ì´í„° ëª¨ë¸
# ============================================================

class SearchSource(str, Enum):
    """ê²€ìƒ‰ ì†ŒìŠ¤"""
    REDDIT = "reddit"
    REDDIT_API = "reddit_api"
    SERPAPI = "serpapi"


class TradePost(BaseModel):
    """ê±°ë˜ ê²Œì‹œê¸€ ëª¨ë¸"""
    # ê¸°ë³¸ ì •ë³´
    title: str = Field(..., description="ì œëª©")
    author: Optional[str] = Field(default=None, description="ì‘ì„±ì")
    author_flair: Optional[str] = Field(default=None, description="ì‘ì„±ì í”Œë ˆì–´")
    
    # ê±°ë˜ ì •ë³´ (ì œëª©ì—ì„œ íŒŒì‹±)
    transaction_type: Optional[str] = Field(default=None, description="ê±°ë˜ ìœ í˜• (WTS/WTB/WTT)")
    country: Optional[str] = Field(default=None, description="êµ­ê°€/ì§€ì—­ ì½”ë“œ")
    
    # ê²Œì‹œê¸€ ì •ë³´
    flair: Optional[str] = Field(default=None, description="ê²Œì‹œê¸€ í”Œë ˆì–´")
    score: int = Field(default=0, description="ì—…ë³´íŠ¸ ìˆ˜")
    comment_count: int = Field(default=0, description="ëŒ“ê¸€ ìˆ˜")
    selftext: str = Field(default="", description="ë³¸ë¬¸ ë‚´ìš©")
    
    # ì‹œê°„ ì •ë³´
    created_timestamp: Optional[datetime] = Field(default=None, description="ì‘ì„± ì‹œê°„")
    scraped_at: datetime = Field(default_factory=datetime.now, description="ìˆ˜ì§‘ ì‹œê°„")
    
    # URL ë° ë¯¸ë””ì–´
    permalink: str = Field(..., description="ê²Œì‹œê¸€ URL")
    first_image_url: Optional[str] = Field(default=None, description="ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL")
    is_gallery: bool = Field(default=False, description="ê°¤ëŸ¬ë¦¬ ì—¬ë¶€")
    
    # ë©”íƒ€ ì •ë³´
    subreddit: Optional[str] = Field(default=None, description="ì„œë¸Œë ˆë”§")
    source: str = Field(default="reddit_api", description="ìˆ˜ì§‘ ì†ŒìŠ¤")


# ============================================================
# Reddit API í´ë˜ìŠ¤
# ============================================================

def parse_title_tags(title: str) -> tuple[Optional[str], Optional[str]]:
    """
    ì œëª©ì—ì„œ ê±°ë˜ ìœ í˜•ê³¼ êµ­ê°€ ì½”ë“œ íŒŒì‹±
    ì˜ˆ: "[WTS][USA] Seventeen photocard" -> ("WTS", "USA")
    """
    import re
    
    # ê±°ë˜ ìœ í˜• íŒ¨í„´
    transaction_types = ["WTS", "WTB", "WTT", "WTT/WTS", "WTS/WTT", "ISO"]
    transaction_type = None
    for tt in transaction_types:
        if tt.lower() in title.lower():
            transaction_type = tt.upper()
            break
    
    # êµ­ê°€ ì½”ë“œ ë° êµ­ê°€ëª… ë§¤í•‘
    country_mapping = {
        # ì½”ë“œ
        "USA": "USA", "US": "USA", "UK": "UK", "EU": "EU", "WW": "WW",
        "CAN": "CAN", "CA": "CAN", "AUS": "AUS", "AU": "AUS",
        "KR": "KR", "JP": "JP", "SG": "SG", "PH": "PH", "MY": "MY",
        "TH": "TH", "ID": "ID", "VN": "VN", "TW": "TW", "HK": "HK",
        "NZ": "NZ", "DE": "DE", "FR": "FR", "NL": "NL", "IT": "IT",
        "ES": "ES", "BR": "BR", "MX": "MX", "IN": "IN",
        # ì „ì²´ êµ­ê°€ëª…
        "CANADA": "CAN", "AUSTRALIA": "AUS", "KOREA": "KR", "JAPAN": "JP",
        "SINGAPORE": "SG", "PHILIPPINES": "PH", "MALAYSIA": "MY",
        "THAILAND": "TH", "INDONESIA": "ID", "VIETNAM": "VN",
        "TAIWAN": "TW", "GERMANY": "DE", "FRANCE": "FR",
        "NETHERLANDS": "NL", "ITALY": "IT", "SPAIN": "ES",
        "BRAZIL": "BR", "MEXICO": "MX", "INDIA": "IN",
    }
    
    # ëŒ€ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    bracket_pattern = r'\[([^\]]+)\]'
    bracket_matches = re.findall(bracket_pattern, title.upper())
    
    country = None
    for match in bracket_matches:
        match_clean = match.strip()
        if match_clean in country_mapping:
            country = country_mapping[match_clean]
            break
    
    return transaction_type, country


class RedditAPIClient:
    """Reddit OAuth API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.app_id = os.getenv("REDDIT_APP_ID")
        self.secret = os.getenv("REDDIT_SECRET")
        self.user_agent = "kpop-trade-collector/2.0.0 (by /u/kpop_collector)"
        self.access_token = None
        self.token_expires_at = None

    def is_available(self) -> bool:
        """Reddit API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return bool(self.app_id and self.secret)

    def authenticate(self) -> bool:
        """Reddit OAuth ì¸ì¦"""
        if not self.is_available():
            return False

        # í† í°ì´ ì•„ì§ ìœ íš¨í•˜ë©´ ì¬ì‚¬ìš©
        if self.access_token and self.token_expires_at:
            if datetime.now() < self.token_expires_at:
                return True

        try:
            auth = requests.auth.HTTPBasicAuth(self.app_id, self.secret)
            data = {
                "grant_type": "client_credentials",
                "device_id": "kpop_trade_collector_v2",
            }
            headers = {"User-Agent": self.user_agent}

            response = requests.post(
                "https://www.reddit.com/api/v1/access_token",
                auth=auth,
                data=data,
                headers=headers,
                timeout=30,
            )
            response.raise_for_status()

            token_data = response.json()
            self.access_token = token_data["access_token"]
            expires_in = token_data.get("expires_in", 3600)
            self.token_expires_at = datetime.now() + timedelta(seconds=expires_in - 60)

            return True

        except Exception as e:
            print(f"  âš ï¸ Reddit ì¸ì¦ ì‹¤íŒ¨: {e}")
            return False

    @retry(
        retry=retry_if_exception_type((requests.exceptions.RequestException, TimeoutError)),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
    )
    def search_subreddit(
        self,
        subreddit: str,
        query: str,
        limit: int = 100,
        sort: str = "relevance",
        time_filter: str = "year",
    ) -> List[TradePost]:
        """ì„œë¸Œë ˆë”§ì—ì„œ ê²€ìƒ‰"""
        if not self.access_token:
            if not self.authenticate():
                return []

        headers = {
            "Authorization": f"bearer {self.access_token}",
            "User-Agent": self.user_agent,
        }

        params = {
            "q": query,
            "limit": min(limit, 100),
            "sort": sort,
            "t": time_filter,
            "restrict_sr": True,
        }

        url = f"https://oauth.reddit.com/r/{subreddit}/search"

        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
        except Exception as e:
            print(f"    âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨ r/{subreddit}: {e}")
            return []

        posts = []
        six_months_ago = datetime.now() - timedelta(days=180)

        for post in data.get("data", {}).get("children", []):
            post_data = post.get("data", {})
            created_at = datetime.fromtimestamp(post_data.get("created_utc", 0))

            # 6ê°œì›” ì´ë‚´ ê²Œì‹œê¸€ë§Œ
            if created_at < six_months_ago:
                continue

            title = post_data.get("title", "")
            transaction_type, country = parse_title_tags(title)
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            first_image_url = None
            is_gallery = post_data.get("is_gallery", False)
            
            if is_gallery and "gallery_data" in post_data:
                # ê°¤ëŸ¬ë¦¬ì¸ ê²½ìš°
                media_metadata = post_data.get("media_metadata", {})
                if media_metadata:
                    first_key = list(media_metadata.keys())[0]
                    first_image_url = media_metadata[first_key].get("s", {}).get("u", "")
            elif post_data.get("url", "").endswith((".jpg", ".png", ".gif", ".jpeg", ".webp")):
                first_image_url = post_data.get("url")
            elif "preview" in post_data:
                images = post_data.get("preview", {}).get("images", [])
                if images:
                    first_image_url = images[0].get("source", {}).get("url", "")
            
            trade_post = TradePost(
                title=title,
                author=post_data.get("author"),
                author_flair=post_data.get("author_flair_text"),
                transaction_type=transaction_type,
                country=country,
                flair=post_data.get("link_flair_text"),
                score=post_data.get("score", 0),
                comment_count=post_data.get("num_comments", 0),
                selftext=post_data.get("selftext", ""),
                created_timestamp=created_at,
                permalink=f"https://reddit.com{post_data.get('permalink', '')}",
                first_image_url=first_image_url,
                is_gallery=is_gallery,
                subreddit=subreddit,
                source="reddit_api",
            )
            posts.append(trade_post)

        return posts

    def get_new_posts(self, subreddit: str, limit: int = 100) -> List[TradePost]:
        """ì„œë¸Œë ˆë”§ ìµœì‹  ê²Œì‹œê¸€ ê°€ì ¸ì˜¤ê¸° (ë‹¨ì¼ í˜ì´ì§€)"""
        posts, _ = self.get_posts_paginated(subreddit, limit=limit, max_pages=1)
        return posts

    def get_posts_paginated(
        self,
        subreddit: str,
        limit: int = 500,
        max_pages: int = 10,
        min_date: Optional[datetime] = None,
    ) -> tuple[List[TradePost], Optional[str]]:
        """
        ì„œë¸Œë ˆë”§ ê²Œì‹œê¸€ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            subreddit: ì„œë¸Œë ˆë”§ ì´ë¦„
            limit: ì´ ê°€ì ¸ì˜¬ ê²Œì‹œê¸€ ìˆ˜
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê° í˜ì´ì§€ 100ê°œ)
            min_date: ì´ ë‚ ì§œ ì´í›„ ê²Œì‹œê¸€ë§Œ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
        
        Returns:
            (ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸, ë§ˆì§€ë§‰ after í† í°)
        """
        if not self.access_token:
            if not self.authenticate():
                return [], None

        headers = {
            "Authorization": f"bearer {self.access_token}",
            "User-Agent": self.user_agent,
        }

        all_posts = []
        after = None
        page = 0

        while len(all_posts) < limit and page < max_pages:
            params = {"limit": 100}
            if after:
                params["after"] = after

            url = f"https://oauth.reddit.com/r/{subreddit}/new"

            try:
                response = requests.get(url, headers=headers, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
            except Exception as e:
                print(f"      âš ï¸ í˜ì´ì§€ {page + 1} ì‹¤íŒ¨: {e}")
                break

            children = data.get("data", {}).get("children", [])
            if not children:
                break

            stop_pagination = False
            for post in children:
                post_data = post.get("data", {})
                created_at = datetime.fromtimestamp(post_data.get("created_utc", 0))

                # ë‚ ì§œ í•„í„° í™•ì¸
                if min_date and created_at < min_date:
                    stop_pagination = True
                    break

                title = post_data.get("title", "")
                transaction_type, country = parse_title_tags(title)
                
                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                first_image_url = None
                is_gallery = post_data.get("is_gallery", False)
                
                if is_gallery and "gallery_data" in post_data:
                    media_metadata = post_data.get("media_metadata", {})
                    if media_metadata:
                        first_key = list(media_metadata.keys())[0]
                        first_image_url = media_metadata[first_key].get("s", {}).get("u", "")
                elif post_data.get("url", "").endswith((".jpg", ".png", ".gif", ".jpeg", ".webp")):
                    first_image_url = post_data.get("url")
                elif "preview" in post_data:
                    images = post_data.get("preview", {}).get("images", [])
                    if images:
                        first_image_url = images[0].get("source", {}).get("url", "")
                
                trade_post = TradePost(
                    title=title,
                    author=post_data.get("author"),
                    author_flair=post_data.get("author_flair_text"),
                    transaction_type=transaction_type,
                    country=country,
                    flair=post_data.get("link_flair_text"),
                    score=post_data.get("score", 0),
                    comment_count=post_data.get("num_comments", 0),
                    selftext=post_data.get("selftext", ""),
                    created_timestamp=created_at,
                    permalink=f"https://reddit.com{post_data.get('permalink', '')}",
                    first_image_url=first_image_url,
                    is_gallery=is_gallery,
                    subreddit=subreddit,
                    source="reddit_api",
                )
                all_posts.append(trade_post)

                if len(all_posts) >= limit:
                    break

            if stop_pagination:
                break

            after = data.get("data", {}).get("after")
            if not after:
                break

            page += 1
            time.sleep(1)  # Rate limit

        return all_posts, after


# ============================================================
# SerpAPI í´ë˜ìŠ¤
# ============================================================

class SerpAPIClient:
    """SerpAPI í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.api_key = os.getenv("SERPAPI_KEY")
        self.base_url = "https://serpapi.com/search"

    def is_available(self) -> bool:
        """SerpAPI ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return bool(self.api_key)

    @retry(
        retry=retry_if_exception_type((requests.exceptions.RequestException, TimeoutError)),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
    )
    def search(
        self,
        query: str,
        language: str = "en",
        max_results: int = 10,
    ) -> List[TradePost]:
        """Google ê²€ìƒ‰ (Reddit ì‚¬ì´íŠ¸ í•„í„°)"""
        if not self.is_available():
            return []

        params = {
            "q": f"{query} site:reddit.com",
            "api_key": self.api_key,
            "num": min(max_results, 100),
            "hl": language,
            "gl": "kr" if language == "ko" else "us",
            "tbs": "qdr:m6",  # ìµœê·¼ 6ê°œì›”
        }

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            if "error" in data:
                print(f"    âš ï¸ SerpAPI ì˜¤ë¥˜: {data.get('error')}")
                return []

        except Exception as e:
            print(f"    âš ï¸ SerpAPI ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

        posts = []
        for item in data.get("organic_results", []):
            title = item.get("title", "")
            transaction_type, country = parse_title_tags(title)
            
            post = TradePost(
                title=title,
                author=None,
                author_flair=None,
                transaction_type=transaction_type,
                country=country,
                flair=None,
                score=0,
                comment_count=0,
                selftext=item.get("snippet", ""),
                created_timestamp=None,
                permalink=item.get("link", ""),
                first_image_url=None,
                is_gallery=False,
                subreddit=None,
                source="serpapi",
            )
            posts.append(post)

        return posts


# ============================================================
# í†µí•© ìˆ˜ì§‘ê¸°
# ============================================================

class KpopTradeCollector:
    """K-pop í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ í†µí•© ìˆ˜ì§‘ê¸°"""

    # K-pop ê±°ë˜ ê´€ë ¨ ì„œë¸Œë ˆë”§
    SUBREDDITS = [
        "kpopforsale",
        "kpopcollections",
        "kpoptrade",
        "adultkpopfans",
    ]

    # ê±°ë˜ í‚¤ì›Œë“œ
    TRADE_KEYWORDS = [
        "wts", "wtb", "wtt", "trade", "trading", "selling", "buying",
        "for sale", "iso", "ì–‘ë„", "íŒë§¤", "êµ¬í•´", "ì‚½ë‹ˆë‹¤", "íŒë‹ˆë‹¤", "êµí™˜"
    ]

    def __init__(self):
        self.reddit = RedditAPIClient()
        self.serpapi = SerpAPIClient()

    def get_search_queries(self, artist: str) -> dict:
        """ì•„í‹°ìŠ¤íŠ¸ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        return {
            "reddit_api": [
                f"{artist} photocard",
                f"{artist} pc",
                f"{artist} WTS",
                f"{artist} WTB",
                f"{artist} WTT",
                f"{artist} trade",
                f"{artist} selling",
            ],
            "serpapi": [
                f"WTS {artist} photocard",
                f"WTB {artist} photocard",
                f"WTT {artist} photocard",
                f"{artist} í¬í† ì¹´ë“œ ì–‘ë„",
                f"kpopforsale {artist}",
            ],
        }

    def is_trade_post(self, post: TradePost) -> bool:
        """ê±°ë˜ ê´€ë ¨ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸"""
        # transaction_typeì´ ì´ë¯¸ íŒŒì‹±ë˜ì–´ ìˆìœ¼ë©´ ê±°ë˜ê¸€
        if post.transaction_type:
            return True
        combined = (post.title + " " + post.selftext).lower()
        return any(kw in combined for kw in self.TRADE_KEYWORDS)

    def contains_artist(self, post: TradePost, artist: str) -> bool:
        """ì•„í‹°ìŠ¤íŠ¸ ê´€ë ¨ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸"""
        artist_lower = artist.lower()
        # ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„ ë³€í˜• (ì˜ˆ: Seventeen -> svt, ì„¸ë¸í‹´)
        artist_variants = [artist_lower]
        
        # ì£¼ìš” ì•„í‹°ìŠ¤íŠ¸ ë³„ëª… ë§¤í•‘
        artist_aliases = {
            "seventeen": ["svt", "ì„¸ë¸í‹´", "sebong"],
            "bts": ["ë°©íƒ„ì†Œë…„ë‹¨", "bangtan"],
            "twice": ["íŠ¸ì™€ì´ìŠ¤"],
            "blackpink": ["ë¸”ë™í•‘í¬", "ë¸”í•‘"],
            "stray kids": ["skz", "ìŠ¤íŠ¸ë ˆì´í‚¤ì¦ˆ", "ìŠ¤í‚¤ì¦ˆ"],
            "newjeans": ["ë‰´ì§„ìŠ¤", "nj"],
            "aespa": ["ì—ìŠ¤íŒŒ"],
            "nct": ["ì—”ì‹œí‹°"],
            "exo": ["ì—‘ì†Œ"],
            "red velvet": ["ë ˆë“œë²¨ë²³", "ë ˆë²¨"],
            "itzy": ["ìˆì§€"],
            "txt": ["íˆ¬ëª¨ë¡œìš°ë°”ì´íˆ¬ê²Œë”", "tomorrow x together"],
            "enhypen": ["ì—”í•˜ì´í”ˆ"],
            "ive": ["ì•„ì´ë¸Œ"],
            "le sserafim": ["ë¥´ì„¸ë¼í•Œ"],
        }
        
        # ë³„ëª… ì¶”ê°€
        if artist_lower in artist_aliases:
            artist_variants.extend(artist_aliases[artist_lower])
        
        combined = (post.title + " " + post.selftext).lower()
        return any(variant in combined for variant in artist_variants)

    def collect_from_reddit_api(
        self,
        artist: Optional[str] = None,
        limit: int = 200,
        max_pages: int = 5,
        months: int = 12,
    ) -> List[TradePost]:
        """
        Reddit APIë¡œ ìˆ˜ì§‘
        
        Args:
            artist: ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„ (Noneì´ë©´ ëª¨ë“  ê±°ë˜ê¸€)
            limit: ì„œë¸Œë ˆë”§ë‹¹ ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜
            max_pages: ì„œë¸Œë ˆë”§ë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            months: ëª‡ ê°œì›” ì „ê¹Œì§€ ìˆ˜ì§‘í• ì§€
        """
        if not self.reddit.is_available():
            print("  âš ï¸ Reddit API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        if not self.reddit.authenticate():
            return []

        print("  âœ… Reddit API ì¸ì¦ ì„±ê³µ")
        
        min_date = datetime.now() - timedelta(days=months * 30)
        print(f"  ğŸ“… ìˆ˜ì§‘ ë²”ìœ„: {min_date.strftime('%Y-%m-%d')} ~ í˜„ì¬ ({months}ê°œì›”)")

        all_posts = []

        # ê° ì„œë¸Œë ˆë”§ì—ì„œ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ìˆ˜ì§‘
        for subreddit in self.SUBREDDITS:
            print(f"\n  ğŸ“ r/{subreddit} (ìµœëŒ€ {max_pages}í˜ì´ì§€)")

            posts, last_after = self.reddit.get_posts_paginated(
                subreddit,
                limit=limit,
                max_pages=max_pages,
                min_date=min_date,
            )
            all_posts.extend(posts)
            
            oldest = min([p.created_timestamp for p in posts if p.created_timestamp], default=None) if posts else None
            oldest_str = oldest.strftime('%Y-%m-%d') if oldest else "N/A"
            print(f"    âœ… {len(posts)} posts (oldest: {oldest_str})")
            
            time.sleep(1)  # Rate limit

        # ì¶”ê°€ë¡œ í‚¤ì›Œë“œ ê²€ìƒ‰ (artistê°€ ì§€ì •ëœ ê²½ìš°)
        if artist:
            queries = self.get_search_queries(artist)["reddit_api"]
            for subreddit in self.SUBREDDITS[:2]:  # ì£¼ìš” 2ê°œë§Œ
                for query in queries[:2]:
                    print(f"    [search] '{query}'...")
                    search_posts = self.reddit.search_subreddit(subreddit, query, limit=50)
                    all_posts.extend(search_posts)
                    print(f"    âœ… {len(search_posts)} posts")
                    time.sleep(1)

        return all_posts

    def collect_from_serpapi(self, artist: str, limit: int = 100) -> List[TradePost]:
        """SerpAPIë¡œ ìˆ˜ì§‘"""
        if not self.serpapi.is_available():
            print("  âš ï¸ SERPAPI_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        print("  âœ… SerpAPI ì‚¬ìš© ê°€ëŠ¥")

        queries = self.get_search_queries(artist)["serpapi"]
        all_posts = []

        for query in queries:
            print(f"    [search] '{query}'...")
            posts = self.serpapi.search(query, language="en", max_results=10)
            all_posts.extend(posts)
            print(f"    âœ… {len(posts)} posts")

        return all_posts

    def collect(
        self,
        artist: Optional[str] = None,
        limit: int = 500,
        source: str = "both",
        max_pages: int = 10,
        months: int = 12,
    ) -> List[TradePost]:
        """
        í†µí•© ìˆ˜ì§‘ ì‹¤í–‰
        
        Args:
            artist: ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„ (Noneì´ë©´ ëª¨ë“  ê±°ë˜ê¸€)
            limit: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            source: ë°ì´í„° ì†ŒìŠ¤ (both, reddit, serpapi)
            max_pages: ì„œë¸Œë ˆë”§ë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            months: ëª‡ ê°œì›” ì „ê¹Œì§€ ìˆ˜ì§‘
        """
        print("=" * 60)
        if artist:
            print(f"ğŸµ {artist} í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ í†µí•© ìˆ˜ì§‘ v2")
        else:
            print("ğŸµ K-pop ì „ì²´ í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ ìˆ˜ì§‘ v2")
        print("=" * 60)
        print(f"ğŸ¯ Target: WTS/WTB/WTT ê±°ë˜ ê²Œì‹œê¸€")
        print(f"ğŸ“Š Limit: ~{limit} posts")
        print(f"ğŸ“… Range: ìµœê·¼ {months}ê°œì›”")
        print(f"ğŸ“„ Pages: ì„œë¸Œë ˆë”§ë‹¹ ìµœëŒ€ {max_pages}í˜ì´ì§€")
        print(f"ğŸ”§ Source: {source}")
        print()

        all_posts = []

        # Reddit API ìˆ˜ì§‘
        if source in ["both", "reddit"]:
            print("ğŸ“¡ [1/2] Reddit API ìˆ˜ì§‘ ì¤‘...")
            reddit_posts = self.collect_from_reddit_api(
                artist=artist,
                limit=limit,
                max_pages=max_pages,
                months=months,
            )
            all_posts.extend(reddit_posts)
            print(f"\n  ğŸ“Š Reddit API ê²°ê³¼: {len(reddit_posts)}ê°œ")

        # SerpAPI ìˆ˜ì§‘ (artistê°€ ì§€ì •ëœ ê²½ìš°ë§Œ)
        if source in ["both", "serpapi"] and artist:
            print("\nğŸ” [2/2] SerpAPI ìˆ˜ì§‘ ì¤‘...")
            serp_posts = self.collect_from_serpapi(artist, limit)
            all_posts.extend(serp_posts)
            print(f"\n  ğŸ“Š SerpAPI ê²°ê³¼: {len(serp_posts)}ê°œ")
        elif source in ["both", "serpapi"] and not artist:
            print("\nğŸ” [2/2] SerpAPI ê±´ë„ˆëœ€ (ì•„í‹°ìŠ¤íŠ¸ ë¯¸ì§€ì • ì‹œ ë¹„íš¨ìœ¨ì )")

        # ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_posts = []
        for post in all_posts:
            normalized_url = post.permalink.rstrip("/")
            if normalized_url not in seen_urls:
                unique_posts.append(post)
                seen_urls.add(normalized_url)

        print(f"\nğŸ“Š ì¤‘ë³µ ì œê±° í›„: {len(unique_posts)}ê°œ")

        # ì•„í‹°ìŠ¤íŠ¸ í•„í„°ë§ (ì§€ì •ëœ ê²½ìš°ë§Œ)
        if artist:
            artist_posts = [p for p in unique_posts if self.contains_artist(p, artist)]
            print(f"ğŸ¤ ì•„í‹°ìŠ¤íŠ¸ '{artist}' í•„í„° í›„: {len(artist_posts)}ê°œ")
        else:
            artist_posts = unique_posts
            print("ğŸ¤ ì•„í‹°ìŠ¤íŠ¸ í•„í„°: ì—†ìŒ (ì „ì²´ ìˆ˜ì§‘)")

        # ê±°ë˜ í‚¤ì›Œë“œ í•„í„°ë§
        trade_posts = [p for p in artist_posts if self.is_trade_post(p)]
        print(f"ğŸ” ê±°ë˜ í‚¤ì›Œë“œ í•„í„° í›„: {len(trade_posts)}ê°œ")

        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        trade_posts.sort(key=lambda p: p.created_timestamp or datetime.min, reverse=True)

        # ì œí•œ ì ìš©
        if len(trade_posts) > limit:
            trade_posts = trade_posts[:limit]

        return trade_posts

    def save_to_jsonl(self, posts: List[TradePost], artist: Optional[str] = None) -> Path:
        """JSONL íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        if artist:
            artist_safe = artist.lower().replace(" ", "_")
            filename = Path("data") / f"{artist_safe}_trade_v2_{timestamp}.jsonl"
        else:
            filename = Path("data") / f"kpop_all_trade_{timestamp}.jsonl"
        Path("data").mkdir(exist_ok=True)

        with open(filename, "w", encoding="utf-8") as f:
            for post in posts:
                data = {
                    "title": post.title,
                    "author": post.author,
                    "author_flair": post.author_flair,
                    "transaction_type": post.transaction_type,
                    "country": post.country,
                    "flair": post.flair,
                    "score": post.score,
                    "comment_count": post.comment_count,
                    "selftext": post.selftext,
                    "created_timestamp": post.created_timestamp.isoformat() if post.created_timestamp else None,
                    "permalink": post.permalink,
                    "first_image_url": post.first_image_url,
                    "is_gallery": post.is_gallery,
                    "scraped_at": post.scraped_at.isoformat(),
                }
                f.write(json.dumps(data, ensure_ascii=False) + "\n")

        return filename


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="K-pop í¬í† ì¹´ë“œ ê±°ë˜ ê²Œì‹œê¸€ í†µí•© ìˆ˜ì§‘ v2",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ì „ì²´ K-pop ê±°ë˜ ê²Œì‹œê¸€ ìˆ˜ì§‘ (ì•„í‹°ìŠ¤íŠ¸ í•„í„° ì—†ìŒ)
  python collect_kpop_trade_v2.py --all
  
  # ì „ì²´ ìˆ˜ì§‘ + ë” ë§ì€ ë°ì´í„° (10í˜ì´ì§€, 12ê°œì›”)
  python collect_kpop_trade_v2.py --all --pages 10 --months 12
  
  # íŠ¹ì • ì•„í‹°ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘
  python collect_kpop_trade_v2.py --artist "Seventeen"
  python collect_kpop_trade_v2.py --artist "BTS"
  
  # ëŒ€ëŸ‰ ìˆ˜ì§‘ (1000ê°œ, 20í˜ì´ì§€, 24ê°œì›”)
  python collect_kpop_trade_v2.py --all --limit 1000 --pages 20 --months 24
        """
    )

    parser.add_argument(
        "--artist",
        type=str,
        default=None,
        help="ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„ (ë¯¸ì§€ì • ì‹œ --all í•„ìš”)",
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="ëª¨ë“  K-pop ê±°ë˜ ê²Œì‹œê¸€ ìˆ˜ì§‘ (ì•„í‹°ìŠ¤íŠ¸ í•„í„° ì—†ìŒ)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=500,
        help="ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ (ê¸°ë³¸: 500)",
    )
    parser.add_argument(
        "--pages",
        type=int,
        default=5,
        help="ì„œë¸Œë ˆë”§ë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 5, í˜ì´ì§€ë‹¹ 100ê°œ)",
    )
    parser.add_argument(
        "--months",
        type=int,
        default=6,
        help="ëª‡ ê°œì›” ì „ê¹Œì§€ ìˆ˜ì§‘ (ê¸°ë³¸: 6)",
    )
    parser.add_argument(
        "--source",
        type=str,
        choices=["both", "reddit", "serpapi"],
        default="reddit",
        help="ë°ì´í„° ì†ŒìŠ¤ (ê¸°ë³¸: reddit)",
    )

    args = parser.parse_args()

    # --all ë˜ëŠ” --artist ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜
    if not args.all and not args.artist:
        print("âŒ --all ë˜ëŠ” --artist ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.")
        print("ì˜ˆì‹œ:")
        print("  python collect_kpop_trade_v2.py --all")
        print("  python collect_kpop_trade_v2.py --artist 'Seventeen'")
        return

    artist = None if args.all else args.artist

    # ìˆ˜ì§‘ ì‹¤í–‰
    collector = KpopTradeCollector()
    posts = collector.collect(
        artist=artist,
        limit=args.limit,
        source=args.source,
        max_pages=args.pages,
        months=args.months,
    )

    if not posts:
        print("\nâŒ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ API í‚¤ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”:")
        print("   - REDDIT_APP_ID, REDDIT_SECRET (.env)")
        print("   - SERPAPI_KEY (.env)")
        return

    # ì €ì¥
    filename = collector.save_to_jsonl(posts, artist)

    print(f"\n{'=' * 60}")
    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(posts)}ê°œ ê±°ë˜ ê²Œì‹œê¸€")
    print(f"ğŸ’¾ ì €ì¥: {filename}")
    print("=" * 60)

    # ì†ŒìŠ¤ë³„ í†µê³„
    sources = {}
    for post in posts:
        sources[post.source] = sources.get(post.source, 0) + 1

    print("\nğŸ“Š ì†ŒìŠ¤ë³„ í†µê³„:")
    for source, count in sorted(sources.items(), key=lambda x: -x[1]):
        print(f"  - {source}: {count}ê°œ")

    # ìƒ˜í”Œ ì¶œë ¥
    print("\nğŸ“‹ ìˆ˜ì§‘ëœ ê±°ë˜ ê²Œì‹œê¸€ ìƒ˜í”Œ:")
    for i, post in enumerate(posts[:10], 1):
        title = post.title[:50] + "..." if len(post.title) > 50 else post.title
        source_tag = f"[{post.source}]"
        print(f"  {i}. {source_tag} {title}")

    if len(posts) > 10:
        print(f"  ... ì™¸ {len(posts) - 10}ê°œ")


if __name__ == "__main__":
    main()

